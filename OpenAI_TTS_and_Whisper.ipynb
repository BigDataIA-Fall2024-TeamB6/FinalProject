{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Python notebook integrates OpenAI's APIs to demonstrate **Text-to-Speech (TTS) **and** Speech-to-Text (STT)** capabilities, enabling seamless interaction with audio and text. The key features include:\n",
        "\n",
        "**Text-to-Speech (TTS):**\n",
        "\n",
        "Converts input text into a lifelike speech audio file using OpenAI's TTS API.\n",
        "Saves the generated audio as an MP3 file and plays it directly in the notebook.\n",
        "**Speech-to-Text (STT):**\n",
        "\n",
        "Transcribes audio files into text using OpenAI's Whisper API (latest interface).\n",
        "Allows users to upload an MP3 file, process it, and display the transcription in a dynamic text box.\n",
        "Interactive Widgets:\n",
        "\n",
        "Utilizes ipywidgets to create an intuitive interface for file uploads and transcription triggers.\n",
        "Displays real-time feedback and outputs directly in the notebook environment.\n",
        "API Integration:\n",
        "\n",
        "Fully integrates OpenAI's APIs with support for Whisper (Audio.transcriptions.create) and TTS models.\n",
        "This notebook serves as a hands-on tool for experimenting with AI-driven audio-to-text and text-to-audio transformations, making it ideal for accessibility enhancements and interactive applications."
      ],
      "metadata": {
        "id": "WQLR80zxmrO2"
      },
      "id": "WQLR80zxmrO2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6836a3",
      "metadata": {
        "id": "bb6836a3"
      },
      "outputs": [],
      "source": [
        "# Required installations (run these in your terminal):\n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78153650",
      "metadata": {
        "id": "78153650",
        "outputId": "72b6684b-2032-4ba8-9497-226db837aefd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5983b4a8",
      "metadata": {
        "id": "5983b4a8",
        "outputId": "13e90cc8-7255-492d-9259-187fa2f8c69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY has been set.\n"
          ]
        }
      ],
      "source": [
        "# Check if OPENAI_API_KEY is already set in the environment\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    # Prompt the user for the API key securely\n",
        "    api_key = getpass(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "    # Set the environment variable\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    print(\"OPENAI_API_KEY has been set.\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY is already set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b25ab3",
      "metadata": {
        "id": "32b25ab3"
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI client\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40612c3",
      "metadata": {
        "id": "d40612c3"
      },
      "outputs": [],
      "source": [
        "def text_to_speech_openai(input_text, output_file_path=\"output_audio.mp3\", voice=\"alloy\", model=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    Convert text to speech using OpenAI's GPT-4o-mini model.\n",
        "    :param input_text: The text to be converted into speech.\n",
        "    :param output_file_path: The path where the audio file will be saved. Default is \"output_audio.mp3\".\n",
        "    :param voice: The voice to use for the TTS. Default is \"alloy\".\n",
        "    :param model: The model to use for TTS. Default is \"gpt-4o-mini\".\n",
        "    :return: The path to the saved audio file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create the audio using the specified TTS model and voice\n",
        "        response = client.audio.speech.create(\n",
        "            model=model,\n",
        "            voice=voice,\n",
        "            input=input_text\n",
        "        )\n",
        "\n",
        "        # Save the generated audio to the specified output file path\n",
        "        response.stream_to_file(output_file_path)\n",
        "\n",
        "        return output_file_path  # Return the path to the saved audio file\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"OpenAI TTS Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f28738",
      "metadata": {
        "id": "f2f28738"
      },
      "outputs": [],
      "source": [
        "def convert_audio_to_text(audio_file_path, model=\"whisper-1\"):\n",
        "    \"\"\"\n",
        "    Convert audio to text using OpenAI's Whisper API.\n",
        "    :param audio_file_path: Path to the audio file to be transcribed.\n",
        "    :param model: The Whisper model to use. Default is \"whisper-1\".\n",
        "    :return: Transcription as text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the audio file in binary mode\n",
        "        with open(audio_file_path, \"rb\") as audio_file:\n",
        "            # Call Whisper API to transcribe the audio file into text\n",
        "            response = client.audio.transcriptions.create(\n",
        "                model=\"whisper-1\",\n",
        "                file=audio_file\n",
        "            )\n",
        "            return response.text  # Extract and return the transcribed text\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"OpenAI Whisper Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77458bc2",
      "metadata": {
        "id": "77458bc2",
        "outputId": "7b08b12d-6299-4268-aff2-9e25e36978e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text-to-Speech audio saved at: gpt4o_audio.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Pigeon\\AppData\\Local\\Temp\\ipykernel_49952\\2229426277.py:19: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
            "  response.stream_to_file(output_file_path)\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Convert Text to Speech\n",
        "text = \"Today is a great day to use OpenAI's GPT-4o-mini for text-to-speech!\"\n",
        "tts_output_path = \"gpt4o_audio.mp3\"\n",
        "try:\n",
        "    # Generate speech from text\n",
        "    tts_audio_file = text_to_speech_openai(input_text=text, output_file_path=tts_output_path, voice=\"alloy\", model=\"tts-1\")\n",
        "    print(f\"Text-to-Speech audio saved at: {tts_audio_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in Text-to-Speech: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33884f9",
      "metadata": {
        "id": "e33884f9",
        "outputId": "66d908d0-f991-4a70-98d5-a9d4637ea0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription: Today is a great day to use OpenAI's GPT-40 Mini for text-to-speech.\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Convert Speech to Text\n",
        "audio_input_path = \"gpt4o_audio.mp3\"  # Path to the audio file to transcribe\n",
        "try:\n",
        "    # Transcribe audio to text\n",
        "    transcription = convert_audio_to_text(audio_file_path=audio_input_path, model=\"whisper-1\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in Speech-to-Text: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastapi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}